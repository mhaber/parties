# Text Preprocessing #
######################
corpus_clean <- tm_map(corpus_clean, gsub, pattern = "\\(.*)", replacement = "") #remove text in brackets
corpus_clean <- tm_map(corpus_clean, stripWhitespace) # remove extra white spaces
corpus_clean <- tm_map(corpus_clean, tolower) # convert words to lower case
corpus_clean <- tm_map(corpus_clean, removePunctuation) # remove punctuation
#corpus_clean <- tm_map(corpus_clean, removeWords, wordlist) #remove unwanted words
#corpus_clean <- tm_map(corpus_clean, removeWords, c("")) # removes indiviual words if not using wordlist.txt
corpus_clean <- tm_map(corpus_clean, removeNumbers) # remove numbers
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("german")) # remove stopwords, Adjust Languages
corpus_clean <- tm_map(corpus_clean,stemDocument, language="german") # stem words, Adjust Languages
corpus_clean <- tm_map(corpus_clean, PlainTextDocument) #convert everything to plain text
### Creating a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus_clean) # creates the Matrix
# inspect(dtm[1:5,100:105]) # lets you inspect the first 5 documents and the 100-105 words
# findFreqTerms(dtm, 50) # list the most frequent words occuring at least 5 times
# dtm<- removeSparseTerms(dtm, 0.4) # remove sparse terms (at least 40%) to reduce size
### Creating a Word-Frequency Matrix
wfm <- wfm(dtm, word.margin = 2)
#######################
# Wordfish Estimation #
#######################
### Run Wordfish
wf.res <- wordfish(wfm, dir=c(2, 44)) #dir=c() is used to anchor documents; i.e. one text has a lower value than another
rm(list=ls(all=T))
library(tm)
library(austin)
library(SnowballC)
######################
# Loading Data Files #
######################
### Set Location of Files
dir="data/sisterParties"
### Load txt files into corpus
corpus=Corpus(DirSource(dir), readerControl=list(reader=readPlain, language="de", load="F")) #Adjust Languages
### Removal Word List
#wordlist <- read.csv("wordlist.txt", stringsAsFactors=F) # wordlist.txt is list of words that you want to exclude.
# Only one word per line, separated by commas
#wordlist=wordlist[,1]
######################
# Text Preprocessing #
######################
corpus_clean <- tm_map(corpus, gsub, pattern = "\\(.*)", replacement = "") #remove text in brackets
corpus_clean <- tm_map(corpus_clean, stripWhitespace) # remove extra white spaces
corpus_clean <- tm_map(corpus_clean, tolower) # convert words to lower case
corpus_clean <- tm_map(corpus_clean, removePunctuation) # remove punctuation
#corpus_clean <- tm_map(corpus_clean, removeWords, wordlist) #remove unwanted words
#corpus_clean <- tm_map(corpus_clean, removeWords, c("")) # removes indiviual words if not using wordlist.txt
corpus_clean <- tm_map(corpus_clean, removeNumbers) # remove numbers
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("german")) # remove stopwords, Adjust Languages
corpus_clean <- tm_map(corpus_clean,stemDocument, language="german") # stem words, Adjust Languages
corpus_clean <- tm_map(corpus_clean, PlainTextDocument) #convert everything to plain text
### Creating a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus_clean) # creates the Matrix
# inspect(dtm[1:5,100:105]) # lets you inspect the first 5 documents and the 100-105 words
# findFreqTerms(dtm, 50) # list the most frequent words occuring at least 5 times
# dtm<- removeSparseTerms(dtm, 0.4) # remove sparse terms (at least 40%) to reduce size
### Creating a Word-Frequency Matrix
wfm <- wfm(dtm, word.margin = 2)
#######################
# Wordfish Estimation #
#######################
### Run Wordfish
wf.res <- wordfish(wfm, dir=c(2, 44)) #dir=c() is used to anchor documents; i.e. one text has a lower value than another
names(corpus)
names(corpus_clean) <- names(corpus)
names(corpus_clean)
### Creating a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus_clean) # creates the Matrix
# inspect(dtm[1:5,100:105]) # lets you inspect the first 5 documents and the 100-105 words
# findFreqTerms(dtm, 50) # list the most frequent words occuring at least 5 times
# dtm<- removeSparseTerms(dtm, 0.4) # remove sparse terms (at least 40%) to reduce size
### Creating a Word-Frequency Matrix
wfm <- wfm(dtm, word.margin = 2)
#######################
# Wordfish Estimation #
#######################
### Run Wordfish
wf.res <- wordfish(wfm, dir=c(2, 44)) #dir=c() is used to anchor documents; i.e. one text has a lower value than another
plot(wf.res.subset, xlab="Wordfish estimates",
main="Positions of CDU and CSU Leaders 1990-2011", col.main="black") # plot subset
plot(wf.res, xlab="Wordfish estimates",
main="Positions of CDU and CSU Leaders 1990-2011", col.main="black") # plot subset
plot(word.coefs$beta[ord],word.coefs$psi[ord], type='n',
xlab="Word Weights",ylab="Word Fixed Effect",
main="Word Fixed Effects and Word Weights of PS Speakers at the 2000 Congress in Grenoble", cex.main = .8)
word.coefs <- coef(wf.res, "poisson")$words # saves word coefficients in the object word.coefs
plot(word.coefs$beta[ord],word.coefs$psi[ord], type='n',
xlab="Word Weights",ylab="Word Fixed Effect",
main="Word Fixed Effects and Word Weights of PS Speakers at the 2000 Congress in Grenoble", cex.main = .8)
word.coefs
head(word.coefs)
findFreqTerms(dtm,5)
findFreqTerms(dtm,10)
findFreqTerms(dtm,20)
head(findFreqTerms(dtm,20))
head(findFreqTerms(dtm,5))
plot(wf.res)
summary(wf.res)
library(ggplot2)
library(ggthemes)
names(summary(wf.res))
names(summary(wf.res$scores))
names(summary(wf.res$model))
names(wf.res)
wf.res$theta
wf.res$docs
wf.res$psi
names(wf.res)
wf.res$call
wf.res$alpha
wf.res$beta
names(wf.res)
wf.res$sigma
wf.res$ll
wf.res$data
names(wf.res)
wf.res$se.theta
names(summary(wf.res))
wf.res$score
wf.res$scores
wf.res$model
names(summary(wf.res$scores))
wf.res$scores
names(wf.res$scores)
summary(wf.res$scores)
summary(wf.res$model)
test <- summary(wf.res)
nmaes(test)
names(test)
test$model
names(test$model)
rm(test)
-1.114694 + 0.03357
-1.114694 + (2 * 0.03357)
-1.114 + (2 * 0.033)
?wordfish
-1.114694 + (1.96 * 0.03357)
-1.114694 - (1.96 * 0.03357)
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$se.theta)
thetas
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
thetas
names(thetas)[3] <- "se"
rownames(thetas) <- NULL
thetas
rownames(thetas) <- NULL
thetas
test <- wf.res$theta - wf.res$se.theta
test
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta + wf.res$se.theta, wf.res$theta - wf.res$se.theta)
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "upper"
names(thetas)[4] <- "lower"
rownames(thetas) <- NULL
thetas
summary(wf.res)
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta + (1.96*wf.res$se.theta),
wf.res$theta - (1.96*wf.res$se.theta))
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "upper"
names(thetas)[4] <- "lower"
rownames(thetas) <- NULL
thetas
thetas$Country <- factor(thetas$Country,
levels = thetas[order(thetas$mean), "Country"]
)
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=21, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Bayesian Factor Analysis")
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta - (1.96*wf.res$se.theta),
wf.res$theta + (1.96*wf.res$se.theta))
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "lower"
names(thetas)[4] <- "upper"
rownames(thetas) <- NULL
thetas$Country <- factor(thetas$Country,
levels = thetas[order(thetas$mean), "Country"]
)
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta - (1.96*wf.res$se.theta),
wf.res$theta + (1.96*wf.res$se.theta))
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "lower"
names(thetas)[4] <- "upper"
rownames(thetas) <- NULL
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=21, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Bayesian Factor Analysis")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=19, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Bayesian Factor Analysis")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Bayesian Factor Analysis")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme_bw(),
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme_classic() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_tufte() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme_classic(axis.text.x = element_text(angle = 90, hjust = 1)
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme_classic(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100))
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3, fill="white") +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_classic() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=3) +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
scale_y_continuous(breaks = c(0,25,50,75,100)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1),
axis.ticks.x=element_blank()) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
theme(axis.text.x = element_text(angle = 90, hjust = 1),
axis.ticks.x=element_blank()) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
### Run Wordfish
wf.res <- wordfish(wfm, dir=c(44, 2)) #dir=c() is used to anchor documents; i.e. one text has a lower value than another
###############
# Diagnostics #
###############
### summary of results
summary(wf.res)
###  Mean, Median and Variance
mean <- mean(wf.res$theta)
median <- median(wf.res$theta)
variance <- var(wf.res$theta)
###  Plot results
# plot(wf.res) # wordfish plot
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta - (1.96*wf.res$se.theta),
wf.res$theta + (1.96*wf.res$se.theta))
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "lower"
names(thetas)[4] <- "upper"
rownames(thetas) <- NULL
### Plot
## To order by score not by document
#thetas$Country <- factor(thetas$Country,
#                         levels = thetas[order(thetas$mean), "Country"])
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
?removeWords
install.packages("tau")
wordlist <- read.csv("data/stopWords.txt", stringsAsFactors=F) # wordlist.txt is list of words that you want to exclude.
wordlist
wordlist=wordlist[,1]
wordlist
rm(list=ls(all=T))
library(tm)
library(austin)
library(SnowballC)
library(ggplot2)
library(ggthemes)
######################
# Loading Data Files #
######################
### Set Location of Files
dir="data/sisterParties"
### Load txt files into corpus
corpus=Corpus(DirSource(dir), readerControl=list(reader=readPlain, language="de", load="F")) #Adjust Languages
### Removal Word List
wordlist <- read.csv("data/stopWords.txt", stringsAsFactors=F) # wordlist.txt is list of words that you want to exclude.
wordlist=wordlist[,1]
######################
# Text Preprocessing #
######################
corpus_clean <- tm_map(corpus, gsub, pattern = "\\(.*)", replacement = "") #remove text in brackets
corpus_clean <- tm_map(corpus_clean, stripWhitespace) # remove extra white spaces
corpus_clean <- tm_map(corpus_clean, tolower) # convert words to lower case
corpus_clean <- tm_map(corpus_clean, removePunctuation) # remove punctuation
corpus_clean <- tm_map(corpus_clean, removeWords, wordlist) #remove unwanted words
#corpus_clean <- tm_map(corpus_clean, removeWords, c("")) # removes indiviual words if not using wordlist.txt
corpus_clean <- tm_map(corpus_clean, removeNumbers) # remove numbers
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("german")) # remove stopwords, Adjust Languages
corpus_clean <- tm_map(corpus_clean,stemDocument, language="german") # stem words, Adjust Languages
corpus_clean <- tm_map(corpus_clean, PlainTextDocument) #convert everything to plain text
names(corpus_clean) <- names(corpus)
### Creating a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus_clean) # creates the Matrix
# inspect(dtm[1:5,100:105]) # lets you inspect the first 5 documents and the 100-105 words
# findFreqTerms(dtm, 50) # list the most frequent words occuring at least 5 times
# dtm<- removeSparseTerms(dtm, 0.4) # remove sparse terms (at least 40%) to reduce size
### Creating a Word-Frequency Matrix
wfm <- wfm(dtm, word.margin = 2)
#######################
# Wordfish Estimation #
#######################
### Run Wordfish
wf.res <- wordfish(wfm, dir=c(44, 2)) #dir=c() is used to anchor documents; i.e. one text has a lower value than another
###############
# Diagnostics #
###############
### summary of results
#summary(wf.res)
###  Mean, Median and Variance
#mean <- mean(wf.res$theta)
#median <- median(wf.res$theta)
#variance <- var(wf.res$theta)
###  Plot results
# plot(wf.res) # wordfish plot
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta - (1.96*wf.res$se.theta),
wf.res$theta + (1.96*wf.res$se.theta))
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "lower"
names(thetas)[4] <- "upper"
rownames(thetas) <- NULL
### Plot
## To order by score not by document
#thetas$Country <- factor(thetas$Country,
#                         levels = thetas[order(thetas$mean), "Country"])
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
inspect(dtm[1:5,100:105])
findFreqTerms(dtm, 50)
dtm<- removeSparseTerms(dtm, 0.4) # remove sparse terms (at least 40%) to reduce size
### Creating a Word-Frequency Matrix
wfm <- wfm(dtm, word.margin = 2)
#######################
# Wordfish Estimation #
#######################
### Run Wordfish
wf.res <- wordfish(wfm, dir=c(44, 2)) #dir=c() is used to anchor documents; i.e. one text has a lower value than another
###############
# Diagnostics #
###############
### summary of results
#summary(wf.res)
###  Mean, Median and Variance
#mean <- mean(wf.res$theta)
#median <- median(wf.res$theta)
#variance <- var(wf.res$theta)
###  Plot results
# plot(wf.res) # wordfish plot
thetas <- data.frame(wf.res$docs, wf.res$theta, wf.res$theta - (1.96*wf.res$se.theta),
wf.res$theta + (1.96*wf.res$se.theta))
names(thetas)[1] <- "Country"
names(thetas)[2] <- "mean"
names(thetas)[3] <- "lower"
names(thetas)[4] <- "upper"
rownames(thetas) <- NULL
### Plot
## To order by score not by document
#thetas$Country <- factor(thetas$Country,
#                         levels = thetas[order(thetas$mean), "Country"])
ggplot(thetas, aes(x=Country, y=mean, group=1)) +
geom_errorbar(width=.1, aes(ymin=lower, ymax=upper)) +
geom_point(shape=20, size=4) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_hline(yintercept = mean(thetas$mean), linetype = "dashed") +
coord_flip() + theme_bw() + xlab("") + ylab("") +
ggtitle("Positions of CDU and CSU Leaders 1990-2011")
rm(list=ls(all=T))
library(tm)
library(austin)
library(SnowballC)
library(ggplot2)
library(ggthemes)
######################
# Loading Data Files #
######################
### Set Location of Files
dir="data/sisterParties"
### Load txt files into corpus
corpus=Corpus(DirSource(dir), readerControl=list(reader=readPlain, language="de", load="F")) #Adjust Languages
### Removal Word List
wordlist <- read.csv("data/stopWords.txt", stringsAsFactors=F) # wordlist.txt is list of words that you want to exclude.
wordlist=wordlist[,1]
######################
# Text Preprocessing #
######################
corpus_clean <- tm_map(corpus, gsub, pattern = "\\(.*)", replacement = "") #remove text in brackets
corpus_clean <- tm_map(corpus_clean, stripWhitespace) # remove extra white spaces
corpus_clean <- tm_map(corpus_clean, tolower) # convert words to lower case
corpus_clean <- tm_map(corpus_clean, removePunctuation) # remove punctuation
corpus_clean <- tm_map(corpus_clean, removeWords, wordlist) #remove unwanted words
#corpus_clean <- tm_map(corpus_clean, removeWords, c("")) # removes indiviual words if not using wordlist.txt
corpus_clean <- tm_map(corpus_clean, removeNumbers) # remove numbers
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("german")) # remove stopwords, Adjust Languages
corpus_clean <- tm_map(corpus_clean,stemDocument, language="german") # stem words, Adjust Languages
corpus_clean <- tm_map(corpus_clean, PlainTextDocument) #convert everything to plain text
names(corpus_clean) <- names(corpus)
### Creating a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus_clean) # creates the Matrix
?removeSparseTerms
?removeWords
length(clean_corpus)
length(corpus_clean)
